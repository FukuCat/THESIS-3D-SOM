%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter_2.tex 
%
%   Description : This file will contain your Review of Related Literature.
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Review of Related Literature}
\label{sec:relatedlit}
This chapter discusses the features, capabilities, and limitations of existing research, algorithms, or software that are related or are similar to the thesis.

\section{Musical Data Representation and Interpretation}

According to Corrêa, D. C., \& Rodrigues, F. A. (2016), as music grows continuously over time, a constant need for an upgrade to satisfy the number and size of music databases causes the development of more accurate tools for music information retrieval (MIR) . MIR is the research field responsible for the development of algorithms or other computational means for the retrieval of useful information from music and the classification of music based on their categories. The ever increasing research on machine learning, the ever expanding abundance of digital audio formats, the growing quality and availability of online symbolic music data, and availability of tools for extracting musical properties motivate this study on machine learning and MIR. One of the main problems in MIR involves the classification of music based on their genre which this study tackles. The automatic genre classification of music plays a key role in online music databases where websites or device music engines manage and label music content for retrieval.

Symbolic-based data are music features extracted from symbolic data formats such as MIDI and KERN. In the MIR community, two main representations of music content for MIR research are followed, either the audio-recorded or the symbolic content. Audio-recorded content produce low-level and middle-level features, whereas symbolic content produce high-level features. When analyzing music content, it is preferable to extract more features with the high-level feature of the symbolic content since it is closer to the human perception of music. Due to these reasons, symbolic-based content is used for the research. This research further provides overviews of important approaches regarding music genre classification with the use of symbolic-based music features. The research, as a result, reveals that pitch and rhythm are the best musical aspects to be explored in symbol-based music feature classification that lead to accurate results.  Some limitations for further improvement on future works however are present such as the small amount of music dataset used in the research, the bias of using western culture music, and the lack of comparison means for the result of the research due to the lack of previous research works regarding symbolic-based music genre classification.

Dubnov, et. al. (2003) formulated that by using statistical and information theoretic tools, one can capture some of the more fundamental trends in musical scores for further analysis. By applying machine learning on these statistical data, one can derive mathematical models for inferring and predicting to a certain extent of generating a “seemingly” new work based on the classical pieces of some popular composers.

Their main source of data for extracting musical surface, a collection of notes for the musical piece, comes in the form of MIDI files or musical instruments digital interface. Machine learning’s primary purpose in this study is to help gather the appropriate data to perform statistical analysis for the usage of applications such as “style characterization tools for the musicologist, generation of stylistic metadata for intelligent retrieval in musical databases, music generation for web and game applications, machine improvisation with or without interaction with humans, and computer-assisted composition”.

Predicting and determining musical context based on relevant past sample is very difficult because the length of the musical context varies widely. Large contexts make it very difficult to estimate because the number of parameters, computational costs, and data requirements for reliable estimation increases exponentially. To address this problem, the usage of predictors that can handle data with very large length is necessary. Two approaches are used to design such a predictor, namely the incremental parsing (IP) and the prefix suffix trees (PST).

The IP algorithm is a lossless coding scheme implying that the application of this algorithm doesn’t result to loss of some spectrum of music while the PST is a lossy compression. The IP also makes sure that every transition is included in the parsing of the music while in PST, the method is very selective in that some rare events and events that do not improve transition are not included. IP uses online estimation through instantaneous coding meaning that IP continually searches online on possible ways or methods to estimate or predict the next possible sequence while in PST, analysis is done by batches through file compression. By analyzing the statistical data provided by either algorithm, predictions or estimates for the next sequence can be made.

Cambouropoulos, E. and Widmer, G. (2000) stated that music could be categorized into small bits called "motives". These motives are extracted from a musical piece by determining which clusters of musical data can be grouped together while maintaining melodic and rhythmic coherence. This is achieved by representing a melodic segment as a series of notes while minding musical closeness. 

Their paper outlines a method that uses differences in pitch-intervals and rhythm as basis for splitting one musical motive from another. For example, two segments can be considered similar if they share a certain number of component notes or intervals using approximate pattern matching. The segments can also be considered similar if they contain shared elements at different pitches. However, this would require a more advanced pattern matching and data structure.

\section{Music Visualization}
Azcarraga \& Flores (2016) made a paper about Visualizing Symphonies using Self Organizing Maps in order to know whether the music of the a certain composer and certain century is influenced by their past counterparts. In the map, there are different parts of it that represents a unique sound. Every time a specific pitch is hit by the music, a line is drawn until the music of a certain composer is finished. The study used traditional machine learning algorithm in order to know whether there is similarity across each century composers. Basically, the study counts the frequency of a certain pitch sound and summarizes it in order to compare it with other composers.

	In order to make a deeper analysis of the study. A new algorithm and variable is going to be used. In the study of this thesis paper, the researchers will add a new dimension and the new variable is a time. The time where a specific pitch will now be important in comparing it to the other pitches of the composers. With this, Time Lapse Algorithm is going to be used in order to summarize data with the time included.

	Azcarraga, A., Caronongan, A., Setiono, R., \& Manalili, S. (2016) presents a variant of the classical 2D SOM that is stable with the general clusters not moving around on every training phase. A structured 3D SOM is an extension of a 2D Self-Organizing Map to 3D with a predefined structure. In their research, the 3D map is represented as a 3x3x3 cube with 27 sub-cubes of the same size. Each sub-cube is further divided into 9x9x9 nodes. The structured 3D SOM is a collection of one distinct core cube in the center and 26 exterior cubes surrounding it, hence summing to a total of 27 sub-cubes. Alongside 3D SOM’s built in structure, the learning algorithm used in this 3D SOM includes a four-phase learning and labelling phase. The first phase of training involves the semi-supervised training of the core cube. The second phase involves yet another semi-supervised training, but for the eight corner cubes. The third phase involves training the core cube again, but the training will be unsupervised. The fourth and final phase will be the labelling phase. This phase involves the uploading of the music files into the cube and labelling them accordingly. The music dataset used in this research includes songs from 9 genres: blues, country, hip-hop, disco, jazz, metal, pop, regae, and rock. Each genre has 100 songs, thus summing to a total of 900 songs.

	SOM is usually represented as a 2D map with the input elements being similar to the input environment. This research verifies that designing the SOM as a 3D map is very feasible, with the learning algorithm still the same as with the 2D map. By extending the SOM from 2D map to 3D, the map is further distinguished into the sub-cubes: eight corner cubes and one core cube in the center. Each corner cube represents a music genre while the core cube represents the song itself. The 3D SOM will be able to identify the quality of the different categories or genres of music albums based on a measure of distortion values of music files with respect to their respective music genres. Distortion value is measured by the Euclidean distance between the core cube and a corner cube.
	
	Foote (1997) presented a paper on Visualizing Music and Audio using Self-Similarity. In this paper, the acoustic similarity between any two instants of an audio file is calculated and displayed as a two-dimensional representation. Structure and repetition is a general feature of nearly all music, with parts resembling certain parts of the song that came before it. This paper presents a method of visualizing the structure of the music by its acoustic similarity or dissimilarity in specific instances of time through grayscale gradation patterns. 
	
	Before getting the similarity measures, the two instants  are first parameterized into Mel-frequency cepstral coefficients (MFCCs) plus an energy term. The similarity measure S(i,j) is computed by getting the autocorrelation of two MFCC feature vectors Vi and Vj that were derived from audio windows. A simple metric of vector similarity S is the scalar product of the vectors. A better similarity measure can be obtained by computing the vector correlation over a window w. This captures the time dependence ofthe vectors. To have high similarity measure, the vectors must not only be similar, but their sequence must be similar as well. 

	Given the similarity measures S(i,j) computed for all window combinations, an image is constructed so that each pixel at location (i,j) is given a grayscale value proportional to the measure. The maximum similarity measure is given maximum brightness. Visually, regions of silence or long sustained notes appear as bright squared on the diagonal. Repeated figures such as choruses and phrases will appear as bright off-diagonal rectangles. If the music has a high degree of repetition, it will show up as diagonal stripes or checkerboards that are offset from the main diagonal. Longer audio files would result to larger images due to the rapid rate of feature vectors. To reduce the image size, the similarity is only calculated for certain time indexes. And since S is already calculated at window size w, the paper only looks at time indexes that are an integer multiple of w.


\newpage
\cite{indiaConf} is an example of Ed

\cite{earth} is an example of Ed