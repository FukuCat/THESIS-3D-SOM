%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter_2.tex 
%
%   Description : This file will contain your Review of Related Literature.
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Review of Related Literature}
\label{sec:relatedlit}
This chapter discusses existing research on musical data representations. It also discusses the application of machine learning in music and visualization techniques for musical compositions. A summary of each section in this chapter is presented prior to the discussion of each section.

\section{Musical Data Representation}
 
\begin{center}
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{4}{|c|}{Musical Data Representation and Interpretation} \\
 \hline
 Authors \&Year &Title&Research Problem&Approach\\
 \hline
 Correa \& Rodrigues (2016)& A survey on symbolic data-based music genre classification & Expanding music database needs more accurate tools for music information retrieval & Symbolic-based music feature are used to train system for genre classification.\\
\hline
McEnnis, McKay, Fujinaga, \& Depalle (2005)& JAudio: A Feature Extraction Library&Solving existing problems in feature extraction systems & They developed jAudio to make extracting features a lot more convenient for researchers.\\
\hline
  Cambouropoulos \& Widmer (2000)& Automated Motivic Analysis via Melodic Clustering & Finding similarity in music patterns. & Their method uses differences in pitch-intervals and rhythm as basis for splitting one musical motive (small bits of music) from another.\\
 \hline
\end{tabular}
\end{center}

As music grows continuously over time, a constant need for an upgrade to satisfy the number and size of music databases causes the development of more accurate tools for music information retrieval (MIR). MIR is the research field responsible for the development of algorithms or other computational means for the retrieval of useful information from music and the classification of music based on their categories. According to CorrÃªa \& Rodrigues (2016), the ever increasing research on machine learning, the ever expanding abundance of digital audio formats, the growing quality and availability of online symbolic music data, and availability of tools for extracting musical properties motivate this study on machine learning and MIR. One of the main problems in MIR involves the classification of music based on their genre which this research work tackles. The automatic genre classification of music plays a key role in online music databases where websites or device music engines manage and label music content for retrieval. The main goal of this research work is to be able to compare music samples and give them their own groups or tags in the database so that they can be easily retrieved whenever needed.

	Symbolic-based data are music features extracted from symbolic data formats such as MIDI and KERN. In the MIR community, two main representations of music content for MIR research are followed, either the audio-recorded or the symbolic content. Audio-recorded content produce low-level and middle-level features, whereas symbolic content produce high-level features. When analyzing music content, it is preferable to extract more features with the high-level feature of the symbolic content since it is closer to the human perception of music. Due to these reasons, symbolic-based content is used for the research. This research further provides overviews of important approaches regarding music genre classification with the use of symbolic-based music features. The research, as a result, reveals that pitch and rhythm are the best musical aspects to be explored in symbol-based music feature classification that lead to accurate results.  Some limitations for further improvement on future works however are present such as the small amount of music dataset used in the research, the bias of using western culture music, and the lack of comparison means for the result of the research due to the lack of previous research works regarding symbolic-based music genre classification.

	McEnnis, McKay, Fujinaga, \& Depalle (2005) introduced a feature extraction software for audio files called jAudio. jAudio provides an easy to use GUI and a command line interface for selecting which features to select/deselect from the list of features in jAudio's current library of feature extraction algorithms which can be found in Appendix C. The software accepts any audio file as input and outputs ACE XML or ARFF format for the features extracted from the audio file. The proponents in this research encountered many problems with regards to existing feature extraction softwares at the time of their research such as there was great difficulty in extracting perceptual features such as meter or pitch from a signal. Another problem was that there was no existing repository of feature extraction algorithms and researchers would have to implement their own feature extraction algorithm whenever they need it and there will be a big chance that they implement the algorithm incorrectly. There was also no existing feature extraction software that produced a standard output format. Feature extraction code was also restricted and not made available to users, thereby denying researchers from developing more feature extraction algorithms.

	JAudio tackles these problems by being a Java-based software, making it easy to acquire and making it compatible with any platform. It produces a standard output format and handles dependencies well by executing all dependencies of a feature extraction algorithm before executing it. For example, the magnitude spectrum of a signal is used by a lot of other features so jAudio would prioritize extracting this first before the others to avoid repeating any extraction process. JAudio also supports metafeatures which are just features that are used by all other features. Examples of this would be derivatives and mean.

Cambouropoulos \& Widmer (2000) stated that music could be categorized into small bits called "motives". These motives are extracted from a musical piece by determining which clusters of musical data can be grouped together while maintaining melodic and rhythmic coherence. This is achieved by representing a melodic segment as a series of notes while minding musical closeness. 

Their paper outlines a method that uses differences in pitch-intervals and rhythm as basis for splitting one musical motive from another. For example, two segments can be considered similar if they share a certain number of component notes or intervals using approximate pattern matching. The segments can also be considered similar if they contain shared elements at different pitches. However, this would require a more advanced pattern matching and data structure.

\section{Machine Learning}

\begin{center}
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{4}{|c|}{Machine Learning} \\
 \hline
 Authors \& Year&Title&Research Problem&Approach\\
 \hline
 Raphael (2010)& Music Plus One and Machine Learning & Computer driven musical accompaniment &Hidden Markov Models and Gaussian Graphical Models\\
\hline
Dubnov, Assayag, Lartillot, \& Bejerano& Using Machine-Learning Methods for Musical Style Modeling& Predicting and determining musical context based on relevant past sample is very difficult because the length of the musical context varies widely & Two approaches, incremental parsing (IP) and the prefix suffix trees (PST), are used in designing predictors that can handle data with very large length.\\
\hline
\end{tabular}
\end{center}

Comparing trends in musical scores and generating a seemingly new work based on the past works of a certain composer has been the focus of another study. In Dubnov, et. al. (2003)'s research, they stated that predicting and determining musical context based on relevant past samples is very difficult because the length of the musical context varies widely. The proponents formulated then that by using statistical and information theoretic tools, one can capture important trends present in the musical scores for further analysis with machine learning to derive mathematical models for inferring and predicting a seemingly new work from this particular composer. Large contexts make it very difficult to estimate because the number of parameters, computational costs, and data requirements for reliable estimation increases exponentially. To address this problem, the usage of predictors that can handle data with very large length is necessary. Two algorithms are used to design such a predictor for generating new works from old music scores, namely the incremental parsing (IP) and the prefix suffix trees (PST).
 
The IP algorithm was first suggested by Ziv \& Lempel (1978). Given a string as input, the algorithm first builds a dictionary of distinct patterns by traversing from left to right of a sequence once and adding to the dictionary every time a new phrase with a different last character from the longest match that already exists in the dictionary. In representing the dictionary with a tree, every node contains a string in the dictionary and each time the algorithm reaches a node, it means that the string input contains the string assigned to the node but is longer. In this case, a new child node will be added to the tree.

PST was developed by Ron, Singer \& Tishby (1996). This algorithm is very similar to IP, but it only adds to its dictionary if and only if the pattern or motif appeared a significant number of times in the string input and will prove to be useful in predicting for the future. Due to this, the main advantage that IP has over PST is that IP is a lossless compression algorithm, since in PST, some patterns are not added to dictionary, especially if they are not significant. PST, however, is more efficient that IP as a parsing algorithm.

	Aside from music comparison, machine learning is also applied in automatic music accompaniment. These accompaniment systems serve as musical partners for live musicians that are performing music that is centered on the soloist. Raphael (2010) developed an accompaniment system with three modules namely âListenâ, âPredictâ, and âPlayâ. The first module interprets the audio input of the live soloist in real-time, identifying note onsets with variable detection latency using hidden Markov model-based score following. However, there will be some detection latency due to the fact that a note must be heard first before it could be identified. To resolve this issue, the Predict module, implements a Gaussian graphical model  that times the accompaniment  on the human musician, continually predicting the evolution as more information comes. 


\section{Music Visualization}

\begin{center}
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{4}{|c|}{Musical Visualization} \\
 \hline
 Authors \& Year&Title&Research Problem&Approach\\
\hline
 Azcarraga, A., Caronongan, A., Setiono, R., \& Manalili, S. (2016) & Validating the Stable Clustering of Songs in a Structured 3D SOM & Will constructing the classic 2D SOM as a 3D map be feasible, with the learning algorithm still the same as the 2D map? & The 3D map is designed as a $3X3X3$ cube with $9X9X9$ nodes. The cube is divided into one core cube and 8 corner cubes. The Euclidean distance from core to each corner represents the quality of the different categories or genres. \\
\hline
Barrington, Chan, \& Lanckriet (2010) & Modelling Music as a Dynamic Texture &Addressing the lack of time-dependency between feature vectors & Dynamic Texture to represent a sequence of audio features \\
\hline
Maaten \& Hinton (2008) & Visualizing Data Using t-SNE &To construct a dimensionality reduction visualization technique that can outperform other existing visualization techniques & Modify SNE to produce a more optimal visualization technique by replacing some steps in the algorithm like the cost function of SNE and by replacing the Gaussian distribution with Student t-distribution \\
 \hline
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{4}{|c|}{Musical Visualization} \\
 \hline
 Authors \& Year&Title&Research Problem&Approach\\
 \hline
 Foote (1997) & Visualizing music and audio using self-similarity & Is it possible to display the acoustic similarity between any two instants of an audio file as a two-dimensional representation & Audio similarity is computed by parameterizing them into MFCC's and getting the autocorrelation of two MFCC feature vectors $V_i$ and $V_j$ that were derived from audio windows. \\
 \hline
\end{tabular}
\end{center}

Modeling music is representing the audio file in a machine-readable form. (Barrington et al., 2010) raises the issue of the lack of time dependency between feature vectors and stresses the need to have the feature vectors ordered in time. When time is ignored, the feature vectors fail to represent the musical dynamics of an audio fragment. The research addresses these limitations and proposes a visualization model for short temporal fragments of music and calls it a dynamic texture.

In another research work regarding the visualization of symphonies using SOM and also the previous research work this research work desires to expand on, Azcarraga \& Flores (2016) focused on whether the music of certain composers and centuries are influenced by prior works of other composers. Their approach relied upon SOMs and k-means clustering where each section on the map represented a specific type of sound. When fed the data from a symphony, a line would be drawn and move from section to section which would represent the different types of sound the SOM would encounter during playback. The result would look like a scribble of lines superimposing each other. By comparing whether this âsignatureâ of the symphony was similar to one of another symphony, the researchers were able to detect the stylistic influence that one composer has with another.

	Azcarraga, Caronongan, Setiono, \& Manalili (2016) presents a variant of the classical 2D SOM, a 3D SOM, that is stable with the general clusters not moving around on every training phase. A structured 3D SOM is an extension of a 2D Self-Organizing Map to 3D with a predefined structure. The 3D SOM is represented as a 3x3x3 cube with 27 sub-cubes of the same size. Each sub-cube is further divided into 9x9x9 nodes. The structured 3D SOM is a collection of one distinct core cube in the center and 26 exterior cubes surrounding it, hence summing to a total of 27 sub-cubes. Alongside 3D SOM's built in structure, the learning algorithm used in this 3D SOM includes a four-phase learning and labelling phase. The first phase of training involves the semi-supervised training of the core cube. The second phase involves yet another semi-supervised training, but for the eight corner cubes. The third phase involves training the core cube again, but the training will be unsupervised. The fourth and final phase will be the labelling phase. This phase involves the uploading of the music files into the cube and labelling them accordingly. The music dataset used in this research includes songs from 9 genres: blues, country, hip-hop, disco, jazz, metal, pop, reggae, and rock. Each genre has 100 songs, thus summing to a total of 900 songs.

	SOM is usually represented as a 2D map with the input elements being similar to the input environment. This research verifies that designing the SOM as a 3D map is very feasible, with the learning algorithm still the same as with the 2D map. By extending the SOM from 2D map to 3D, the map is further distinguished into the sub-cubes: eight corner cubes and one core cube in the center. Each corner cube represents a music genre while the core cube represents the song itself. The 3D SOM will be able to identify the quality of the different categories or genres of music albums based on a measure of distortion values of music files with respect to their respective music genres. Distortion value is measured by the Euclidean distance between the core cube and a corner cube.

	Maaten \& Hinton presents a visualization technique using dimensional reduction of high dimensional data into a 2D or 3D map called t-SNE as was briefly discussed in the introduction. This technique aims to transform high dimensional data into low dimensional data representation for data plotting in the map using a series of computational steps or algorithm which will be discussed in more detail in Chapter 3.

	In their research work, they compared t-SNE with other techniques for dimensionality reduction for visualization of data which includes Sammon mapping, Isomap, and LLE using the MNIST data set, the Olivetti faces data set, and the COIL-20 data set. The resulting visualizations by t-SNE for all three types of data set proved to be superior to the three other techniques as t-SNE was able to cleanly cluster the different data classes together for each data set as compared to the other three techniques.

	The main advantage of using t-SNE to other techniques is that t-SNE models dissimilar data points by means of large pairwise distances and models similar data points by means of small pairwise distances. This would result in a visual image that have similar data points grouped together and are far apart from data points that are very dissimilar with them. T-SNE also uses either of two tricks which they label as early compression and early exaggeration. Early compression is when the map points are forced to stay together at the start of the optimization and early exaggeration is when map points are forced to have large gaps between their respective clusters by multiplying all of the high dimensional probabilities with a certain constant value so that the modelled data points will have larger values. When the data points are closely packed together in early compression, the clusters will be able to move much easier. Similarly, when there are large gaps among the different data points, the clusters will also be able to much easier to find a good global optimization.

	
	Foote (1997) presented a paper on Visualizing Music and Audio using Self-Similarity. In this paper, the acoustic similarity between any two instants of an audio file is calculated and displayed as a two-dimensional representation. Structure and repetition is a general feature of nearly all music, with parts resembling certain parts of the song that came before it. This paper presents a method of visualizing the structure of the music by its acoustic similarity or dissimilarity in specific instances of time through grayscale gradation patterns. 
	
	Before getting the similarity measures, the two instants  are first parameterized into Mel-frequency cepstral coefficients (MFCCs) plus an energy term. The similarity measure $S(i,j)$ is computed by getting the autocorrelation of two MFCC feature vectors $V_i$ and $V_j$ that were derived from audio windows. A simple metric of vector similarity S is the scalar product of the vectors. A better similarity measure can be obtained by computing the vector correlation over a window w. This captures the time dependence ofthe vectors. To have high similarity measure, the vectors must not only be similar, but their sequence must be similar as well. 

	Given the similarity measures $S(i,j)$ computed for all window combinations, an image is constructed so that each pixel at location $(i,j)$ is given a grayscale value proportional to the measure. The maximum similarity measure is given maximum brightness. Visually, regions of silence or long sustained notes appear as bright squared on the diagonal. Repeated figures such as choruses and phrases will appear as bright off-diagonal rectangles. If the music has a high degree of repetition, it will show up as diagonal stripes or checkerboards that are offset from the main diagonal. Longer audio files would result to larger images due to the rapid rate of feature vectors. To reduce the image size, the similarity is only calculated for certain time indexes and since S is already calculated at window size w, the paper only looks at time indexes that are an integer multiple of w.